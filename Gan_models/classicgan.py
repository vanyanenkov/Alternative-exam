# -*- coding: utf-8 -*-
"""classicGan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yAJJrXx362oO-O82v8JRwIMvKIGtYXhF
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d

from sklearn.metrics import mean_squared_error, r2_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

n = 1000

first_column = torch.rand(n, 1).to(device)

columns = [first_column]
for i in range(1, 10):
    columns.append(2 * columns[-1])  # Each new column is twice the previous one

# Combine columns to form data (10 columns in total)
data = torch.cat(columns, dim=1)

# Generator
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(10, 100),
            nn.ReLU(),
            nn.Linear(100, 100),
            nn.ReLU(),
            nn.Linear(100, 10)
        )

    def forward(self, x):
        return self.model(x)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(10, 100),
            nn.ReLU(),
            nn.Linear(100, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Initialize the models and move them to the device
generator = Generator().to(device)
discriminator = Discriminator().to(device)

# Loss and optimizers
criterion = nn.BCELoss()
optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.001)
optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.001)

d_loss_values = []
g_loss_values = []

# Training the GAN
num_epochs = 50000
for epoch in range(num_epochs):
    # Train discriminator
    optimizer_d.zero_grad()

    real_data = data
    real_labels = torch.ones(n, 1).to(device)
    outputs = discriminator(real_data)
    d_loss_real = criterion(outputs, real_labels)

    # Generate fake data
    noise = torch.randn(n, 10).to(device)
    fake_data = generator(noise)
    fake_labels = torch.zeros(n, 1).to(device)
    outputs = discriminator(fake_data.detach())
    d_loss_fake = criterion(outputs, fake_labels)

    # Backprop and optimize
    d_loss = d_loss_real + d_loss_fake
    d_loss.backward()
    optimizer_d.step()

    # Train generator
    optimizer_g.zero_grad()
    outputs = discriminator(fake_data)
    g_loss = criterion(outputs, real_labels)
    g_loss.backward()
    optimizer_g.step()

    d_loss_values.append(d_loss.item())
    g_loss_values.append(g_loss.item())

    # Print losses
    if (epoch+1) % 2500 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}")

d_loss_values_plt = []
g_loss_values_plt = []
for i in range(len(d_loss_values)):
    if i % 100 == 0:
        d_loss_values_plt.append(d_loss_values[i])
        g_loss_values_plt.append(g_loss_values[i])

# Plot losses
plt.figure(figsize=(10, 5))
plt.plot(range(len(d_loss_values_plt)), d_loss_values_plt, label='Discriminator Loss')
plt.plot(range(len(g_loss_values_plt)), g_loss_values_plt, label='Generator Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Losses of Generator and Discriminator')
plt.legend()
plt.show()

# Plot smoothed losses
plt.figure(figsize=(10, 5))
smoothed_d_loss = gaussian_filter1d(d_loss_values_plt, sigma=2)
smoothed_g_loss = gaussian_filter1d(g_loss_values_plt, sigma=2)
plt.plot(range(len(smoothed_d_loss)), smoothed_d_loss, label='Discriminator Loss')
plt.plot(range(len(smoothed_g_loss)), smoothed_g_loss, label='Generator Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Smoothed Training Losses of Generator and Discriminator')
plt.legend()
plt.show()

# After training, generate some synthetic data
with torch.no_grad():
    test_noise = torch.randn(n, 10).to(device)
    generated_data = generator(test_noise).cpu().numpy()

# Print the first 10 rows of generated data
print("Generated Data (First 10 rows):")
for i in range(10):
    print(generated_data[i])

# To validate if relationships hold:
print("\nValidation (For the first 10 rows):")
for i in range(10):
    print(f"First: {generated_data[i][0]:.4f}, Expected Second: {2*generated_data[i][0]:.4f}, Actual Second: {generated_data[i][1]:.4f}")
    print(f"Second: {generated_data[i][1]:.4f}, Expected Third: {2*generated_data[i][1]:.4f}, Actual Third: {generated_data[i][2]:.4f}")
    print(f"Third: {generated_data[i][2]:.4f}, Expected Fourth: {2*generated_data[i][2]:.4f}, Actual Fourth: {generated_data[i][3]:.4f}")
    print(f"Fourth: {generated_data[i][3]:.4f}, Expected Fifth: {2*generated_data[i][3]:.4f}, Actual Fifth: {generated_data[i][4]:.4f}")
    print(f"Fifth: {generated_data[i][4]:.4f}, Expected Sixth: {2*generated_data[i][4]:.4f}, Actual Sixth: {generated_data[i][5]:.4f}")
    print(f"Sixth: {generated_data[i][5]:.4f}, Expected Seventh: {2*generated_data[i][5]:.4f}, Actual Seventh: {generated_data[i][6]:.4f}")
    print(f"Seventh: {generated_data[i][6]:.4f}, Expected Eighth: {2*generated_data[i][6]:.4f}, Actual Eighth: {generated_data[i][7]:.4f}")
    print(f"Eighth: {generated_data[i][7]:.4f}, Expected Ninth: {2*generated_data[i][7]:.4f}, Actual Ninth: {generated_data[i][8]:.4f}")
    print(f"Ninth: {generated_data[i][8]:.4f}, Expected Tenth: {2*generated_data[i][8]:.4f}, Actual Tenth: {generated_data[i][9]:.4f}\n")


def evaluate_generator(generated_data):
    # Calculate the expected data (based on the rule that each column is 2x the previous one)
    expected_data = np.zeros_like(generated_data)
    expected_data[:, 0] = generated_data[:, 0]

    for i in range(1, 10):
        expected_data[:, i] = 2 * expected_data[:, i - 1]

    mse = np.mean((generated_data - expected_data) ** 2)

    max_mse = np.max((expected_data) ** 2)
    quality = 100 * (1 - mse / max_mse)

    return quality


# Evaluate the generator's performance
generator_quality = evaluate_generator(generated_data)
print(f"Generator Quality: {generator_quality:.2f}%")

plt.figure(figsize=(10, 5))
plt.hist(real_data[:, 0], bins=50, alpha=0.5, label='Real Data', color='blue')
plt.hist(generated_data[:, 0], bins=50, alpha=0.5, label='Generated Data', color='orange')
plt.title('Distribution of the First Column')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

# Plot comparison of real and generated data for each column
for i in range(real_data.shape[1]):
    plt.figure(figsize=(10, 5))
    plt.hist(real_data[:, i], bins=50, alpha=0.5, label='Real Data', color='blue')
    plt.hist(generated_data[:, i], bins=50, alpha=0.5, label='Generated Data', color='orange')
    plt.title(f'Distribution of Column {i+1}')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

# Validate relationships for the first 10 rows
expected_data = np.zeros_like(generated_data)
expected_data[:, 0] = generated_data[:, 0]

expected_data_1 = np.zeros_like(generated_data)
expected_data_1[:, 0] = generated_data[:, 0]


for i in range(1, 10):
    expected_data[:, i] = 2 * expected_data[:, i - 1]
    expected_data_1[:, i] = 2 * expected_data[:, 0]


for i in range(10):
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, 11), generated_data[i], label='Generated Data', marker='o', linestyle='-', color='blue')
    plt.plot(range(1, 11), expected_data[i], label='Expected Data', marker='x', linestyle='--', color='red')
    # plt.plot(range(1, 11), expected_data_1[i], label='Expected Data_2', marker='o', linestyle='-', color='green')
    plt.title(f'Validation for Row {i + 1}')
    plt.xlabel('Column Index')
    plt.ylabel('Value')
    plt.legend()
    plt.grid()
    plt.show()