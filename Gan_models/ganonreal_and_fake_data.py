# -*- coding: utf-8 -*-
"""GaNonreal and fake data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WefohlHxv9CVw7CRz1s8VRPA0-lyfSSv

###Пример сравнение модели, обученной на реальных данных и модели, обученной на синтетических данных, видно, что accuracy упирается в архитектуру модели. Отличие в том, что у сгенерированных данных большк потеря, но не сильно.
"""

!pip install ctgan

from ctgan import CTGAN
import pandas as pd
import torch
import torch.nn as nn
from torch import optim
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from torch.nn import Sigmoid, BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, functional, Tanh

import numpy as np

class Simple_model(nn.Module):
    def __init__(self, input_dim):
        super(Simple_model, self).__init__()
        self.layers = nn.Sequential(
            Linear(input_dim, 128),
            ReLU(),
            Dropout(0.2),
            Linear(128, 128),
            ReLU(),
            Dropout(0.2),
            Linear(128, 64),
            ReLU(),
            Dropout(0.2),
            Linear(64, 1),
            Sigmoid()
        )

    def forward(self, x):
        return self.layers(x)

categorical_features = ['Age','Gender','Color','Transparency','Glucose','Protein','WBC','RBC','Epithelial Cells','Mucous Threads','Amorphous Urates','Bacteria']

categorical_features = ['Gender','Color','Transparency','Glucose','Protein','Epithelial Cells','Mucous Threads','Amorphous Urates','Bacteria']

real_data = pd.read_csv('urinaset (2).csv')
real_data = real_data.iloc[:, 1:]

label_encoders = {}

def labelencoder(df):
    label_encoders = {}  # Словарь для хранения LabelEncoder для каждого столбца
    for c in df.columns:
        if df[c].dtype == 'object':
            df[c] = df[c].fillna('N')
            lbl = LabelEncoder()
            lbl.fit(list(df[c].values))
            df[c] = lbl.transform(df[c].values)
            label_encoders[c] = lbl
    return df, label_encoders

real_data, label_encoders =labelencoder(real_data)

def labeldecoder(df, label_encoders):
    for c in df.columns:
        if c in label_encoders:
            lbl = label_encoders[c]
            df[c] = lbl.inverse_transform(df[c].values)
    return df

ctgan = CTGAN(verbose=True)
ctgan.fit(real_data, categorical_features, epochs = 200)

samples = ctgan.sample(1000)
decoded_data = labeldecoder(samples, label_encoders)
decoded_data.to_csv('data.csv', index=True)

# real_data = pd.read_csv('/urinaset (2).csv')
# real_data = real_data.iloc[:, 1:]
fake_data = decoded_data

def labelencoder(df):
    label_encoders = {}  # Словарь для хранения LabelEncoder для каждого столбца
    for c in df.columns:
        if df[c].dtype == 'object':
            df[c] = df[c].fillna('N')
            lbl = LabelEncoder()
            lbl.fit(list(df[c].values))
            df[c] = lbl.transform(df[c].values)
            label_encoders[c] = lbl  # Сохраняем объект LabelEncoder
    return df, label_encoders

real_data, label_encoders =labelencoder(real_data)
fake_data, label_encoders =labelencoder(fake_data)

df_x_train_real, df_x_test_real, df_y_train_real, df_y_test_real = train_test_split(
    real_data.drop("Diagnosis", axis=1),
    real_data["Diagnosis"],
    test_size=0.2,
)
batch_size = 32

# Преобразование данных в тензоры
X_train_real = torch.tensor(df_x_train_real.values.astype(np.float32), dtype=torch.float32)
X_test_real = torch.tensor(df_x_test_real.values.astype(np.float32), dtype=torch.float32)
y_train_real = torch.tensor(df_y_train_real.values, dtype=torch.float32).unsqueeze(1)
y_test_real = torch.tensor(df_y_test_real.values, dtype=torch.float32).unsqueeze(1)

train_dataset_real = TensorDataset(X_train_real, y_train_real)
test_dataset_real = TensorDataset(X_test_real, y_test_real)

train_loader_real = DataLoader(train_dataset_real, batch_size=batch_size, shuffle=True)
test_loader_real = DataLoader(test_dataset_real, batch_size=batch_size, shuffle=False)


df_x_train_fake, df_x_test_fake, df_y_train_fake, df_y_test_fake = train_test_split(
    fake_data.drop("Diagnosis", axis=1),
    fake_data["Diagnosis"],
    test_size=0.2,
)

# Преобразование данных в тензоры
X_train_fake= torch.tensor(df_x_train_fake.values.astype(np.float32), dtype=torch.float32)
X_test_fake= torch.tensor(df_x_test_fake.values.astype(np.float32), dtype=torch.float32)
y_train_fake = torch.tensor(df_y_train_fake.values, dtype=torch.float32).unsqueeze(1)
y_test_fake = torch.tensor(df_y_test_fake.values, dtype=torch.float32).unsqueeze(1)

train_dataset_fake = TensorDataset(X_train_fake, y_train_fake)
test_dataset_fake = TensorDataset(X_test_fake, y_test_fake)

train_loader_fake = DataLoader(train_dataset_fake, batch_size=batch_size, shuffle=True)
test_loader_fake = DataLoader(test_dataset_fake, batch_size=batch_size, shuffle=False)

input_dim = df_x_train_real.shape[1]  # Размерность входных данных

model_on_real_data = Simple_model(input_dim)

optimizer = optim.Adam(model_on_real_data.parameters())
criterion = nn.BCELoss()  # Binary Cross Entropy Loss

num_epochs = 25
for epoch in range(num_epochs):
    model_on_real_data.train()
    running_loss = 0.0
    for batch_X, batch_y in train_loader_real:
        optimizer.zero_grad()
        outputs = model_on_real_data(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

model_on_real_data.eval()
with torch.no_grad():
    test_loss = 0.0
    correct = 0
    total = 0
    for batch_X, batch_y in test_loader_real:
        outputs = model_on_real_data(batch_X)
        loss = criterion(outputs, batch_y)
        test_loss += loss.item()
        predicted = (outputs >= 0.5).float()
        correct += (predicted == batch_y).sum().item()
        total += batch_y.size(0)

accuracy = correct / total
print(f"Test real data Loss: {test_loss / len(test_loader_real):.4f}")
print(f"Test real data Accuracy: {accuracy:.4f}")

model_on_fake_data = Simple_model(input_dim)

optimizer = optim.Adam(model_on_fake_data.parameters())
criterion = nn.BCELoss()  # Binary Cross Entropy Loss

num_epochs = 25
for epoch in range(num_epochs):
    model_on_fake_data.train()
    running_loss = 0.0
    for batch_X, batch_y in train_loader_fake:
        optimizer.zero_grad()
        outputs = model_on_fake_data(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

model_on_fake_data.eval()
with torch.no_grad():
    test_loss = 0.0
    correct = 0
    total = 0
    for batch_X, batch_y in test_loader_real:
        outputs = model_on_fake_data(batch_X)
        loss = criterion(outputs, batch_y)
        test_loss += loss.item()
        predicted = (outputs >= 0.5).float()
        correct += (predicted == batch_y).sum().item()
        total += batch_y.size(0)

accuracy = correct / total
print(f"Test fake data Loss: {test_loss / len(test_loader_real):.4f}")
print(f"Test fake data Accuracy: {accuracy:.4f}")